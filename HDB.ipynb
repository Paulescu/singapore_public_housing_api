{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting sale price of flat\n",
    "\n",
    "## Background\n",
    "In Singapore, close to 80% of the population live in public housing. These are heavily subsidised by the government and as such, strict rules have been implemented to ensure the subsidies are not being misused. One such rule is that every couple is not to own more than a single subsidised property at any one time. \n",
    "\n",
    "## Project Motivation\n",
    "I am interested in finding the potential selling price of my flat in 3 years. The main objective of this project is to provide an estimate of the expected retail price for the flat in due time. In this project, I will use a variety of regression models to make predictions on the potential selling price, rather than to take the prediction of a single model. This is to showcase the relative effectiveness of the various models for the present purpose -- predict sale price of a flat given historical transaction data.\n",
    "\n",
    "## Data Collection\n",
    "Transaction data of public flats have been dilligently chronicled [here](https://data.gov.sg/dataset/resale-flat-prices). In the final version of the project, the data will be obtained from a json object from an API call. In the initial version(s), the entire dataset will be downloaded and fed to the project in a `pandas` dataframe. Operations such as cleaning and grouping will then be performed on the dataframe itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspiration from GovTech DSAID blog\n",
    "\n",
    "Before going on to the project I would like to acknowledge that a huge part of this project is inspired from a [blog](https://medium.com/dsaid-govtech/wanted-data-scientists-how-we-designed-our-first-case-interview-9fd4eaa4607c) written by the [Data Science and Artificial Intelligence Division (DSAID)](https://medium.com/dsaid-govtech) of a governmental agency called GovTech. They have been pushed to the forefront since the COVID-19 pandemic hit Singapore. A familiar (and valuable) product developed by DSAID was the TraceTogether Token (and its associated App). \n",
    "\n",
    "For those who are unfamiliar, the token collects data via bluetooth signals, such as proximity data, duration in proximity data and places visited (We are supposed to scan for entry at the entrance of facilities like shopping malls). In the early days, this token expidited contact tracing of people exposed to others who were subsequently identified as COVID-positive, facilitating rapid isolation to supress the spread of the virus. If an individual had been shown to be in the proximity of a (subsequently identified) positive individual using data from the token, he/she will also receive a message to perform health monitoring for the requisite number of days.\n",
    "\n",
    "After seeing the impact they have on the life on Singaporeans, after launching useful products like the TraceTogether Token and App, I thought what they are doing is very meaningful and creates impact in our everyday lives. I am sure the job there will be sufficiently motivating in and of itself.\n",
    "\n",
    "I would want to start off with using the OneMap API to geocode the lat-long coordinates just for the fun of it. Intuitively, I do not think that should affect the selling price anymore than the 'Town' feature. Including it is purely for practice on calling information from APIs.\n",
    "\n",
    "First, the essential libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as req\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the information on [HDB API](https://data.gov.sg/dataset/resale-flat-prices), we can find the API query string as follows. We will perform our entire analysis using our entire dataset of 92,270 entries(correct as of the time of writing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_str = 'https://data.gov.sg/api/action/datastore_search?resource_id=f1765b54-a209-4718-8d38-a39237f502b3&limit=500'\n",
    "response = req.get(query_str)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A side note on the response obtained from the **requests** library. A *status_code* of 200 implies that the request was successful. Otherwise the response code would have been *404 NOT FOUND*. Other possible status codes can be found [here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(response.content)\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['result']['records'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "As mentioned, in the initial version(s), the data is obtained from a downloaded `.csv` file and read into a dataframe. This will be changed to an API call in the final version. After that, check that the dataset has been properly read and explore some features of the data such as the number of columns, the number of rows and the type of every column.\n",
    "\n",
    "Even though data from the 1990's are available, they are probably outdated due to the numerous policy implementations over the years. We will only use the most recent data from 2017 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief explanation of columns\n",
    "The various dates within the dataset may appear strage to someone not staying in Singapore. Most HDB flats come with a default 99-year lease from the government. At the end of the 99 years, the government reserve the right to repossess the land on which the flat is built for other developments. This is largely due to the lack of geographic mass of the country, which drives up the price of land and properties.\n",
    "\n",
    "An additional note is that the `month` column contains data on when the associated flat is sold on the second-hand market. Feel free to refer to the metadata file for explanations to the context of other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are a total of 131560 entries, which should be enough for a small project.\n",
    "\n",
    "### Data type of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, most of the columns are classified as `object`, even though some of them contains useful numerical data, such as `flat_type`, `storey_range` and `month`. With all else being equal, the higher the value of `storey_range`, the higher the resale price. This is due to Singaporeans' inherent preference for flats on a higher level. In this section, we will transform them into the appropriate forms.\n",
    "\n",
    "First we extract the `year` and `month` value from the resale date, and thereafter dropping it since our model does not take in `datetime` type as a feature.\n",
    "\n",
    "Columns such as `town`, `flat_type` and `flat_model` will be One-hot encoded as they contain information about different categories of flats, with no inherent order. Features like `storey_range` on the other land will be transformed using `OrdinalEncoder` since the value of the category has an effect on the resale price (take it from a Singaporean who has stayed here all his life!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.rename({'month': 'sale_date'}, axis=1, inplace=True)\n",
    "df['sale_date'] = pd.to_datetime(df['month'], format=\"%Y-%m\")\n",
    "df['sale_year'] = df['sale_date'].dt.year\n",
    "df['sale_month'] = df['sale_date'].dt.month\n",
    "df.drop(['sale_date', 'month'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling `remaining_lease`\n",
    "The amount of remaining lease affects the resale price in an indirect way. As mentioned, the maximum lease for HDB flats is currently at 99 years. Hence, the longer the remaining lease, the more valuable the flat is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['years_remaining'] = df['remaining_lease'].apply(lambda x: int(x.split()[0]))\n",
    "df['months_remaining'] = df['remaining_lease'].apply(lambda x: int(x.split(\" \")[-2]))\n",
    "df.drop([\"remaining_lease\", \"lease_commence_date\"], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['storey_range'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "storey_cat = ['01 TO 03', '04 TO 06', '07 TO 09', '10 TO 12', '13 TO 15', '16 TO 18', \n",
    "                '19 TO 21', '22 TO 24', '25 TO 27', '28 TO 30', '31 TO 33', '34 TO 36',\n",
    "                '37 TO 39', '40 TO 42', '43 TO 45', '46 TO 48', '49 TO 51']\n",
    "oe = OrdinalEncoder(categories=[storey_cat], dtype=int)\n",
    "df[\"storey\"] = oe.fit_transform(df[[\"storey_range\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(dtype=int)\n",
    "ohe_features = ['town', 'flat_type', 'flat_model']\n",
    "flat_type_encoding = ohe.fit_transform(df[ohe_features]).toarray()\n",
    "flat_type_encoding_df = pd.DataFrame(flat_type_encoding, columns=ohe.get_feature_names())\n",
    "df = df.join(flat_type_encoding_df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we work on encoding the other columns, such as `town` and `flat_model`, once again using `OneHotEncoder`. However, `storey_range` will be encoded with `OrdinalEncoder` instead due to its ordinal nature. In general, the higher the level, the better the resale price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the `df.info()`, we see that most of our dataframe columns contain objects. We need to handle them one by one. Thanks to the data collection and meticulous efforts, there are no missing values in the columns, and this means we do not have to worry about filling in the *NaN*'s with mean or median values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['town'].unique())\n",
    "df['town'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our data set contains around 26 unique town locations. Given the property mantra of \"location, location, location\", the idea will be to tag them by the first 3 digits of their postal code to turn the data in this column from `object` to `int` so as to be fed into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.distplot(df['resale_price'])\n",
    "plt.xlabel('Resale Price (S$M)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from finding out how the prices are distributed, we would also want to find out how the other features correlate with the resale price. This is so that we can sort the features in terms of the correlation it has with the resale price subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMat = df.corr()\n",
    "sns.heatmap(corrMat, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are not many cells to plot here since the correlation can only be calculated for numerical features. Here we see that amongst the numerical features, **SIZE DOES MATTER**! \n",
    "\n",
    "Of course the correlation of PSM should be largely the same as floor area since it is derived as the ratio of the resale price and floor area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.boxplot(x=df['town'], y=df['resale_price']/1E6)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n",
    "ax.set_ylabel('Resale Price (S$M)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this plot looks messy since it is arranged in alphabetical order of the town the flat resides in. Let's try to order this by the median price instead and see if the plot looks tidier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resale_median = df.groupby(['town']).median().reset_index()\n",
    "resale_median.sort_values(by='resale_price', ascending=True, inplace=True, ignore_index=True)\n",
    "resale_median['resale_price'] /= 1000000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='town', y='resale_price', data=resale_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"town\", \"flat_type\", \"flat_model\", \"storey_range\", \"block\", \"street_name\"], axis=1, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('resale_price', axis=1), df['resale_price'], test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concluding Thoughts\n",
    "The mean_squared_error of $59.3B translates to a RMS error of around $24.3K. Given that the resale prices across all flat types ranges around $300 - 600K, this translates to an error of almost 10%. The error can be further reduced by properly setting the hyperparameters of the `LinearRegression` model, or processing the features with `PolynomialFeatures` before fitting. \n",
    "\n",
    "In the interim, let's recall the initial motivation for this project -- to find the projected price of my flat given the relevant features. I shall do that below, but for a 4-room flat at the \"01 TO 03\" storey range in Yishun, does anyone want to make a guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myflat = pd.read_csv(\"data/myflat.csv\", header=0)\n",
    "myflat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myflat['sale_year'] = myflat['month'].apply(lambda x: int(x.split(\"-\")[0]))\n",
    "myflat['sale_month'] = myflat['month'].apply(lambda x: int(x.split(\"-\")[-1]))\n",
    "myflat.drop('month', axis=1, inplace=True)\n",
    "myflat['storey'] = oe.transform(myflat[[\"storey_range\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myflat_encoding = ohe.transform(myflat[ohe_features]).toarray()\n",
    "myflat_encoding_df = pd.DataFrame(myflat_encoding, columns=ohe.get_feature_names())\n",
    "myflat = myflat.join(myflat_encoding_df)\n",
    "myflat['years_remaining'] = myflat['remaining_lease'].apply(lambda x: int(x.split()[0]))\n",
    "myflat['months_remaining'] = myflat['remaining_lease'].apply(lambda x: int(x.split(\" \")[-2]))\n",
    "myflat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myflat.drop([\"remaining_lease\", \"lease_commence_date\", \"town\", \"flat_type\", \"flat_model\", \"storey_range\", \"block\", \"street_name\"], axis=1, inplace=True)\n",
    "myflat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df.columns) == len(myflat.columns) + 1\n",
    "myflat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myflat = myflat[X_train.columns]    # switch the order of columns in myflat to match that of X_train\n",
    "print(f\"\\nPredicted price for my flat is ${lr.predict(myflat)[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! The model predicted a sale price of around $483K, which seems to match the records of recent transactions around the area (as of July 2022). I will make a mental note to update the actual transacted price once I sell my flat, so do remember to star this project to be kept in the loop. And remember, this comes even without including additional information such as proximity to transport hubs, famous schools and venues of entertainment such as shopping hubs. I would say that it is pretty impressive! \n",
    "\n",
    "What do you think?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('messy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8831b77c9561aafa6266b23e0c893f5800518ee262b8fc72f9e180a43c94a18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
